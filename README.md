## [Machine Reading Comprehension(MRC) Datasets](#mrc) 

## [2013](#2013)
* 1/1 [MCTest](#MCTest)
  * MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text [paper](https://mattr1.github.io/mctest/MCTest_EMNLP2013.pdf) or [here](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/MCTest_EMNLP2013.pdf) or [here](http://aclweb.org/anthology/D13-1020), [dataset](https://mattr1.github.io/mctest/data.html), [homepage](https://mattr1.github.io/mctest/index.html)


## [2014](#2014)
* 2/1 [ProcessBank](#ProcessBank)
  * Modeling Biological Processes for Reading Comprehension [paper](https://nlp.stanford.edu/pubs/berant-srikumar-manning-emnlp14.pdf), [homepage&code&dataset](https://nlp.stanford.edu/software/bioprocess/)

## [2015](#2015)
* 3/1 [bAbI](#bAbI)
* 4/2 [CNN/Daily Mail](#CNNDailyMail)
* 5/3 [CBTest](#CBTest)

## [2016](#2016)
* 6/1 [ROCStories](#ROCStories)
* 7/2 [SQuAD1.0](#SQuAD1.0)
* 8/3 [LAMBADA](#LAMBADA)
* 9/4 [SelQA](#SelQA)
* 10/5 [WikiReading](#WikiReading)
* 11/6 [WDW](#WDW)
* 12/7 [MS MARCO](#MS MARCO)
* 13/8 [NewsQA](#NewsQA)


## [2017](#2017)
* 14/1 [SCT](#SCT)
* 15/2 [RACE](#RACE)
* 16/3 [SearchQA](#SearchQA)
* 17/4 [TriviaQA](#TriviaQA)
* 18/5 [Quasar](#Quasar)
* 19/6 [WikiHop&MedHop](#WikiHop&MedHop)
* 20/7 [CLOTH](#CLOTH)
* 21/8 [NarrativeQA](#NarrativeQA)

## [2018](#2018)
* 22/1 [MCScript](#MCScript)
* 23/2 [ARC](#ARC)
* 24/3 [CliCR](#CliCR)
* 25/4 [DuoRC](#DuoRC)
* 26/5 [EmoryRCDC](#EmoryRCDC)
* 27/6 [SQuAD2.0](#SQuAD2.0)
  * Know What You Don't Know: Unanswerable Questions for SQuAD [paper](https://arxiv.org/abs/1806.03822), [home](https://rajpurkar.github.io/SQuAD-explorer/)
* 28/7 [QuAC](#QuAC)
* 29/8 [CoQA](#CoQA)
* 30/9 [HotpotQA](#HotpotQA)
* 31/10 [ReCoRD](#ReCoRD)



## [2019](#2019)
* 32/1 [Google's Natural Questions](#NQ)

## To be continued...

<hr>

## [中文机器阅读理解数据集](#cmrc) <br> Chinese Machine Reading Comprehension(CMRC) Datasets

## [2016](#c2016)
* [PD&CFT](#PDCFT)
  * Consensus Attention-based Neural Networks for Chinese Reading Comprehension [paper](https://arxiv.org/abs/1607.02250), [dataset](https://github.com/ymcui/Chinese-RC-Dataset)

## [2017](#c2017)
* [The First Evaluation Workshop on Chinese Machine Reading Comprehension (CMRC 2017)](#CMRC2017) 
  * Dataset for the First Evaluation on Chinese Machine Reading Comprehension [paper](https://arxiv.org/abs/1709.08299), [dataset](https://github.com/ymcui/cmrc2017)

* [DuReader](#DuReader)
  * DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications [paper](https://arxiv.org/abs/1711.05073), [dataset&baseline](https://github.com/baidu/DuReader), [homepage](https://ai.baidu.com/broad/subordinate?dataset=dureader)

## [2018](#c2018)
* [The Second Evaluation Workshop on Chinese Machine Reading Comprehension (CMRC 2018)](CMRC2018)
  * A Span-Extraction Dataset for Chinese Machine Reading Comprehension [paper](https://arxiv.org/abs/1810.07366), [dataset](https://github.com/ymcui/cmrc2018)

* [DRCD](#DRCD) (Traditional Chinese machine reading comprehension)
  * DRCD: a Chinese Machine Reading Comprehension Dataset [paper](https://arxiv.org/abs/1806.00920), [dataset](https://github.com/DRCSolutionService/DRCD)
* [ODSQA](#ODSQA) (Traditional Chinese machine reading comprehension)
  * ODSQA: Open-domain Spoken Question Answering Dataset [paper](https://arxiv.org/abs/1808.02280), [dataset](https://github.com/chiahsuan156/ODSQA)

<hr>

## <span id="mrc">Machine Reading Comprehension(MRC) Datasets</span>

## <span id="2013">2013</span>

### <span id="MCTest">MCTest</span>


## <span id="2014">2014</span>

### <span id="ProcessBank">ProcessBank</span>


## <span id="20153">2015</span>

### <span id="bAbI">bAbI</span>

### <span id="CNNDailyMail">CNN/Daily Mail</span>

### <span id="CBTest">CBTest</span>


## <span id="2016">2016</span>

### <span id="ROCStories">ROCStories</span>

### <span id="SQuAD1.0">SQuAD1.0</span>

### <span id="LAMBADA">LAMBADA</span>

### <span id="SelQA">SelQA</span>

### <span id="WikiReading">WikiReading</span>

### <span id="WDW">WDW</span>

### <span id="MS MARCO">MS MARCO</span>

### <span id="NewsQA">NewsQA</span>


## <span id="2017">2017</span>

### <span id="SCT">SCT</span>

### <span id="RACE">RACE</span>

### <span id="SearchQA">SearchQA</span>

### <span id="TriviaQA">TriviaQA</span>

### <span id="Quasar">Quasar</span>

### <span id="WikiHop&MedHop">WikiHop&MedHop</span>

### <span id="CLOTH">CLOTH</span>

### <span id="NarrativeQA">NarrativeQA</span>


## <span id="2018">2018</span>

### <span id="MCScript">MCScript</span>

### <span id="ARC">ARC</span>

### <span id="CliCR">CliCR</span>

### <span id="DuoRC">DuoRC</span>

### <span id="EmoryRCDC">EmoryRCDC</span>

### <span id="SQuAD2.0">SQuAD2.0</span>

### <span id="QuAC">QuAC</span>

### <span id="CoQA">CoQA</span>

### <span id="HotpotQA">HotpotQA</span>

### <span id="ReCoRD">ReCoRD</span>


## <span id="2019">2019</span>

### <span id="NQ">Google's Natural Questions</span>

<hr>

## <span id="cmrc">中文机器阅读理解数据集</span> <br> Chinese Machine Reading Comprehension(CMRC) Datasets

## <span id="c2016">2016</span>

### <span id="PDCFT">PD&CFT</span>

## <span id="c2017">2017</span>

### <span id="CMRC2017">The First Evaluation Workshop on Chinese Machine Reading Comprehension (CMRC 2017)</span>

### <span id="DuReader">DuReader</span>

## <span id="c2018">2018</span>

### <span id="CMRC2018">The Second Evaluation Workshop on Chinese Machine Reading Comprehension (CMRC 2018)</span>

### <span id="DRCD">DRCD</span>

### <span id="ODSQA">ODSQA</span>
